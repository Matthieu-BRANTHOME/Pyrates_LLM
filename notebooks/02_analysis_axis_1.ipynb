{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "![Logo Pyrates LLM](../assets/pyratesllm_logo_500.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# **Notebook#02 : Analysis on axis 1**\n",
    "## [Impact of the digital assistant on the learner]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1/ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import students_constants as stu_const\n",
    "import interaction_constants as int_const\n",
    "import tests_constants  as tes_const\n",
    "\n",
    "# External\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2/ Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_data = pd.read_pickle(\"../data/interim/interaction_data.pkl\")\n",
    "pre_test_data = pd.read_pickle(\"../data/interim/pre_test_data.pkl\")\n",
    "post_test_data = pd.read_pickle(\"../data/interim/post_test_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3/ Game progression (Q1.1) [A/B/C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### 3.1/ Calculate game progression (index 0-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all STARTED actions to find maximum level reached by each student\n",
    "started_actions = interaction_data[\n",
    "    interaction_data[int_const.ACTION_DATA_KEY] == int_const.STARTED_ACTION\n",
    "]\n",
    "\n",
    "# Find maximum level reached by each student\n",
    "max_level_per_student = started_actions.groupby(\n",
    "    int_const.GAME_ID_DATA_KEY\n",
    ")[int_const.LEVEL_DATA_KEY].max()\n",
    "\n",
    "print(f\"Number of games detected: {len(max_level_per_student)}\")\n",
    "\n",
    "student_progress_data = []\n",
    "\n",
    "for game_id in max_level_per_student.index:\n",
    "    max_level = max_level_per_student[game_id]\n",
    "    \n",
    "    # Get all traces for this student at his maximum level\n",
    "    student_level_data = interaction_data[\n",
    "        (interaction_data[int_const.GAME_ID_DATA_KEY] == game_id) & \n",
    "        (interaction_data[int_const.LEVEL_DATA_KEY] == max_level)\n",
    "    ]\n",
    "    \n",
    "    # Maximum progression\n",
    "    max_progression = student_level_data[int_const.GAME_PROGRESSION_DATA_KEY].max()\n",
    "    \n",
    "    # Group id\n",
    "    group = student_level_data[int_const.GROUP_ID_DATA_KEY].iloc[0]\n",
    "\n",
    "    # Results aggregation \n",
    "    student_progress_data.append({\n",
    "        'game_id': game_id,\n",
    "        'group_id': group,\n",
    "        'max_level': max_level,\n",
    "        'max_progression': max_progression\n",
    "    })\n",
    "\n",
    "# Create progress dataframe\n",
    "progress_df = pd.DataFrame(student_progress_data)\n",
    "\n",
    "# Calculate progress index (0-100 points)\n",
    "# - Each completed level gives 12.5 points (100/8)\n",
    "# - For current level, progression percentage gives fraction of 12.5 points\n",
    "def calculate_progress_index(max_level, max_progression):\n",
    "    completed_levels_points = (max_level - 1) * (100 / 8)\n",
    "    current_level_points = (max_progression / 100) * (100 / 8)\n",
    "    return min(completed_levels_points + current_level_points, 100)\n",
    "\n",
    "progress_df['progress_index'] = progress_df.apply(\n",
    "    lambda row: calculate_progress_index(row['max_level'], row['max_progression']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"Number of progress entries computed: {len(progress_df)}\")\n",
    "# Export to Excel for debug\n",
    "progress_df.to_excel(\"../debug/debug_students_game_progression.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### 3.2/ Test A VS (B+C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Is there a difference in game progression between students from group A and students from group B+C ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract progress data for each group\n",
    "group_a_progress = progress_df[progress_df['group_id'] == 'A']['progress_index']\n",
    "group_b_progress = progress_df[progress_df['group_id'] == 'B']['progress_index']\n",
    "group_c_progress = progress_df[progress_df['group_id'] == 'C']['progress_index']\n",
    "# Combine B and C into one group\n",
    "group_bc_progress = pd.concat([group_b_progress, group_c_progress])\n",
    "\n",
    "# Descriptive statistics\n",
    "average_progress_A = group_a_progress.mean()\n",
    "average_progress_BC = group_bc_progress.mean()\n",
    "\n",
    "std_progress_A = group_a_progress.std()\n",
    "std_progress_BC = group_bc_progress.std()\n",
    "\n",
    "mean_diff = average_progress_BC - average_progress_A\n",
    "percent_increase = (mean_diff / average_progress_A) * 100\n",
    "\n",
    "print(\"=== DESCRIPTIVE STATISTICS ===\")\n",
    "print(f\"Group A: n = {len(group_a_progress)}, mean = {average_progress_A:.2f}, std = {std_progress_A:.2f}\")\n",
    "print(f\"Group B+C: n = {len(group_bc_progress)}, mean = {average_progress_BC:.2f}, std = {std_progress_BC:.2f}\")\n",
    "print(f\".   Difference in mean (B+C - A): {mean_diff:.2f}\")\n",
    "print(f\".   Percentage increase from Group A: {percent_increase:.2f}%\")\n",
    "\n",
    "alpha = 0.05\n",
    "print(f\"\\n=== NORMALITY TESTS (Shapiro-Wilk) ===\")\n",
    "shapiro_A = stats.shapiro(group_a_progress)\n",
    "shapiro_BC = stats.shapiro(group_bc_progress)\n",
    "print(f\"Shapiro A: p-value = {shapiro_A.pvalue:.4f}\")\n",
    "print(f\"Shapiro B+C: p-value = {shapiro_BC.pvalue:.4f}\")\n",
    "\n",
    "# Normality conclusions\n",
    "if shapiro_A.pvalue >= alpha:\n",
    "    print(\"Group A: data appears normally distributed\")\n",
    "else:\n",
    "    print(\"Group A: data does not appear normally distributed\")\n",
    "\n",
    "if shapiro_BC.pvalue >= alpha:\n",
    "    print(\"Group B+C: data appears normally distributed\")\n",
    "else:\n",
    "    print(\"Group B+C: data does not appear normally distributed\")\n",
    "\n",
    "print(f\"\\n=== HOMOGENEITY OF VARIANCES (Levene) ===\")\n",
    "levene_p = stats.levene(group_a_progress, group_bc_progress).pvalue\n",
    "print(f\"Levene A vs B/C p-value = {levene_p:.4f}\")\n",
    "\n",
    "# Variance conclusion\n",
    "if levene_p >= alpha:\n",
    "    print(\"Variances are similar between groups\")\n",
    "else:\n",
    "    print(\"Variances differ between groups\")\n",
    "\n",
    "print(f\"\\n=== TEST RESULTS ===\")\n",
    "# Decide which test to use\n",
    "normal_A = shapiro_A.pvalue >= alpha\n",
    "normal_BC = shapiro_BC.pvalue >= alpha\n",
    "levene_AB_C = levene_p >= alpha\n",
    "\n",
    "if normal_A and normal_BC and levene_AB_C:\n",
    "    print(\"Using independent t-test (parametric)\")\n",
    "    t_stat, p_value = stats.ttest_ind(group_a_progress, group_bc_progress, equal_var=True)\n",
    "\n",
    "    # Calculate Cohen's d (pooled standard deviation)\n",
    "    n1, n2 = len(group_a_progress), len(group_bc_progress)\n",
    "    pooled_std = np.sqrt(((n1 - 1) * std_progress_A**2 + (n2 - 1) * std_progress_BC**2) / (n1 + n2 - 2))\n",
    "    cohens_d = mean_diff / pooled_std\n",
    "    \n",
    "    print(f\"Test statistic (t) = {t_stat:.4f}\")\n",
    "    print(f\"P-value = {p_value:.4f}\")\n",
    "    print(f\"Cohen's d = {cohens_d:.4f}\")\n",
    "    \n",
    "    # Interpret effect size\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect_interpretation = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect_interpretation = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect_interpretation = \"medium\"\n",
    "    else:\n",
    "        effect_interpretation = \"large\"\n",
    "    print(f\"Effect size: {effect_interpretation}\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Using Mann-Whitney U test (non-parametric)\")\n",
    "    t_stat, p_value = stats.mannwhitneyu(group_a_progress, group_bc_progress, alternative='two-sided')\n",
    "\n",
    "    # Calculate effect size r = Z / sqrt(N)\n",
    "    n_total = len(group_a_progress) + len(group_bc_progress)\n",
    "    z_score = stats.norm.ppf(1 - p_value / 2)  # Convert p-value to z-score\n",
    "    r_effect = z_score / np.sqrt(n_total)\n",
    "    \n",
    "    print(f\"Test statistic (U) = {t_stat:.4f}\")\n",
    "    print(f\"P-value = {p_value:.4f}\")\n",
    "    print(f\"Effect size (r) = {r_effect:.4f}\")\n",
    "    \n",
    "    # Interpret effect size\n",
    "    if abs(r_effect) < 0.1:\n",
    "        effect_interpretation = \"negligible\"\n",
    "    elif abs(r_effect) < 0.3:\n",
    "        effect_interpretation = \"small\"\n",
    "    elif abs(r_effect) < 0.5:\n",
    "        effect_interpretation = \"medium\"\n",
    "    else:\n",
    "        effect_interpretation = \"large\"\n",
    "    print(f\"Effect size: {effect_interpretation}\")\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(f\"SIGNIFICANT: p-value = {p_value:.4f} < alpha = {alpha}\")\n",
    "    print(\"There are statistically significant differences between Group A and Group B+C\")\n",
    "else:\n",
    "    print(f\"NOT SIGNIFICANT: p-value = {p_value:.4f} > alpha = {alpha}\")\n",
    "    print(\"No statistically significant differences between Group A and Group B+C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 3.3/ Test A VS B VS C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Is there a difference in game progression between students from groups A, B, and C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "average_progress_A = group_a_progress.mean()\n",
    "average_progress_B = group_b_progress.mean()\n",
    "average_progress_C = group_c_progress.mean()\n",
    "\n",
    "diff_B = average_progress_B - average_progress_A\n",
    "percent_increase_B = (diff_B / average_progress_A) * 100\n",
    "\n",
    "diff_C = average_progress_C - average_progress_A\n",
    "percent_increase_C = (diff_C / average_progress_A) * 100\n",
    "\n",
    "\n",
    "print(\"=== DESCRIPTIVE STATISTICS ===\")\n",
    "print(f\"Group A: n = {len(group_a_progress)}, mean = {average_progress_A:.2f}, std = {group_a_progress.std():.2f}\")\n",
    "print(f\"Group B: n = {len(group_b_progress)}, mean = {average_progress_B:.2f}, std = {group_b_progress.std():.2f}\")\n",
    "print(f\".   diff vs A = {diff_B:.2f}, increase = {percent_increase_B:.2f}%\")\n",
    "print(f\"Group C: n = {len(group_c_progress)}, mean = {average_progress_C:.2f}, std = {group_c_progress.std():.2f}\")\n",
    "print(f\".   diff vs A = {diff_C:.2f}, increase = {percent_increase_C:.2f}%\")\n",
    "\n",
    "alpha = 0.05\n",
    "print(f\"=== NORMALITY TESTS (Shapiro-Wilk) ===\")\n",
    "shapiro_A = stats.shapiro(group_a_progress)\n",
    "shapiro_B = stats.shapiro(group_b_progress)\n",
    "shapiro_C = stats.shapiro(group_c_progress)\n",
    "\n",
    "print(f\"Group A: p-value = {shapiro_A.pvalue:.4f}\")\n",
    "print(f\"Group B: p-value = {shapiro_B.pvalue:.4f}\")\n",
    "print(f\"Group C: p-value = {shapiro_C.pvalue:.4f}\")\n",
    "\n",
    "# Check if all groups are normally distributed\n",
    "all_normal = all(p >= alpha for p in [shapiro_A.pvalue, shapiro_B.pvalue, shapiro_C.pvalue])\n",
    "\n",
    "if all_normal:\n",
    "    print(\"All groups follow normal distribution -> ANOVA is applicable.\")\n",
    "else:\n",
    "    print(\"At least one group is not normally distributed -> Prefer Kruskal-Wallis test.\")\n",
    "\n",
    "print(f\"\\n=== HOMOGENEITY OF VARIANCES (Levene) ===\")\n",
    "levene_test = stats.levene(group_a_progress, group_b_progress, group_c_progress)\n",
    "print(f\"Levene's test for homogeneity of variances: p-value = {levene_test.pvalue:.4f}\")\n",
    "\n",
    "if levene_test.pvalue >= alpha:\n",
    "    print(\"Variances are homogeneous\")\n",
    "else:\n",
    "    print(\"Variances are not homogeneous\")\n",
    "\n",
    "\n",
    "# Prepare data for overall tests\n",
    "all_progress = pd.concat([\n",
    "    pd.DataFrame({'group': 'A', 'progress': group_a_progress}),\n",
    "    pd.DataFrame({'group': 'B', 'progress': group_b_progress}),\n",
    "    pd.DataFrame({'group': 'C', 'progress': group_c_progress})\n",
    "])\n",
    "\n",
    "# ANOVA (parametric)\n",
    "anova_result = stats.f_oneway(group_a_progress, group_b_progress, group_c_progress)\n",
    "# One-way : one factor (group)\n",
    "print(f\"\\n=== ONE-WAY ANOVA RESULTS ===\")\n",
    "print(f\"F-statistic: {anova_result.statistic:.4f}\")\n",
    "print(f\"P-value: {anova_result.pvalue:.4f}\")\n",
    "\n",
    "# Kruskal-Wallis test (non-parametric)\n",
    "kruskal_result = stats.kruskal(group_a_progress, group_b_progress, group_c_progress)\n",
    "print(f\"\\n=== KRUSKAL-WALLIS TEST RESULTS ===\")\n",
    "print(f\"H-statistic: {kruskal_result.statistic:.4f}\")\n",
    "print(f\"P-value: {kruskal_result.pvalue:.4f}\")\n",
    "\n",
    "# Interpretation of overall test\n",
    "if all_normal and levene_test.pvalue >= alpha:\n",
    "    overall_pvalue = anova_result.pvalue\n",
    "    test_used = \"ANOVA\"\n",
    "else:\n",
    "    overall_pvalue = kruskal_result.pvalue\n",
    "    test_used = \"Kruskal-Wallis\"\n",
    "\n",
    "print(f\"\\n=== OVERALL INTERPRETATION (using {test_used}) ===\")\n",
    "if overall_pvalue < alpha:\n",
    "    print(f\"SIGNIFICANT: p-value = {overall_pvalue:.4f} < alpha = {alpha}\")\n",
    "    print(\"There are statistically significant differences between at least two groups\")\n",
    "else:\n",
    "    print(f\"NOT SIGNIFICANT: p-value = {overall_pvalue:.4f} > alpha = {alpha}\")\n",
    "    print(\"No statistically significant differences between groups\")\n",
    "\n",
    "# POST-HOC TESTS (if significant overall difference)\n",
    "if overall_pvalue < alpha:\n",
    "    print(f\"\\n=== POST-HOC PAIRWISE COMPARISONS ===\")\n",
    "    \n",
    "    # For parametric data: Tukey HSD\n",
    "    if test_used == \"ANOVA\":\n",
    "        print(\"Tukey HSD Post-hoc Test:\")\n",
    "        tukey = pairwise_tukeyhsd(endog=all_progress['progress'], groups=all_progress['group'], alpha=alpha)\n",
    "        print(tukey)\n",
    "        \n",
    "        # Extract significant pairs\n",
    "        print(\"\\nSignificant pairwise differences:\")\n",
    "        for i in range(len(tukey._results[0])):\n",
    "            if tukey._results[4][i]:  # If reject (True)\n",
    "                group1, group2 = tukey._results[1][i], tukey._results[2][i]\n",
    "                meandiff = tukey._results[3][i]\n",
    "                pval = tukey._results[4][i]\n",
    "                print(f\"  {group1} vs {group2}: SIGNIFICANT (mean diff = {meandiff:.2f}, p = {pval:.4f})\")\n",
    "    \n",
    "    # For non-parametric data: Mann-Whitney U tests with Bonferroni correction\n",
    "    else:\n",
    "        print(\"Mann-Whitney U tests with Bonferroni correction:\")\n",
    "        pairs = [('A', 'B'), ('A', 'C'), ('B', 'C')]\n",
    "        bonferroni_alpha = alpha / len(pairs)  # Bonferroni correction\n",
    "        \n",
    "        for pair in pairs:\n",
    "            group1, group2 = pair\n",
    "            data1 = group_a_progress if group1 == 'A' else (group_b_progress if group1 == 'B' else group_c_progress)\n",
    "            data2 = group_a_progress if group2 == 'A' else (group_b_progress if group2 == 'B' else group_c_progress)\n",
    "            \n",
    "            u_stat, p_value = stats.mannwhitneyu(data1, data2)\n",
    "            adjusted_p = p_value * len(pairs)  # Bonferroni adjustment\n",
    "            significance = \"SIGNIFICANT\" if adjusted_p < alpha else \"not significant\"\n",
    "            \n",
    "            print(f\"  {group1} vs {group2}: p = {p_value:.4f}, \"\n",
    "                  f\"adjusted p = {adjusted_p:.4f} -> {significance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 4/ Learning gain (Q1.2) [A/B/C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### 4.1/ Learning gain calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "The learning gain calculation is based on ANSWERS_SCORES dictionary (see `scr/tests_constants.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate maximum score\n",
    "max_score_general = sum(max(answer.values()) for answer in tes_const.ANSWERS_SCORES.values())\n",
    "\n",
    "# Score calculation function\n",
    "def calculate_score(student, max_score):\n",
    "    score = 0\n",
    "    for question, answer in student.items():\n",
    "        if question in tes_const.ANSWERS_SCORES and answer in tes_const.ANSWERS_SCORES[question]:\n",
    "            score += tes_const.ANSWERS_SCORES[question][answer]\n",
    "    return round((score/max_score)*100)\n",
    "\n",
    "groups = [tes_const.GROUP_A, tes_const.GROUP_B, tes_const.GROUP_C]\n",
    "scores_data = {}\n",
    "\n",
    "for group in groups:\n",
    "    # Filter and calculate scores\n",
    "    pre_temp = pre_test_data[pre_test_data[tes_const.GROUP_ID_KEY] == group].copy()\n",
    "    post_temp = post_test_data[post_test_data[tes_const.GROUP_ID_KEY] == group].copy()\n",
    "    \n",
    "    pre_temp['pre_score'] = pre_temp.apply(calculate_score, axis=1, max_score=max_score_general)\n",
    "    post_temp['post_score'] = post_temp.apply(calculate_score, axis=1, max_score=max_score_general)\n",
    "    \n",
    "    # Merge and calculate gain\n",
    "    scores_df = pd.merge(\n",
    "        pre_temp[[tes_const.STUDENT_ID_KEY, 'pre_score']],\n",
    "        post_temp[[tes_const.STUDENT_ID_KEY, 'post_score']],\n",
    "        on=tes_const.STUDENT_ID_KEY,\n",
    "        how='inner' # the student must have a pre-test score AND a post-test score\n",
    "    )\n",
    "    scores_df['learning_gain'] = scores_df['post_score'] - scores_df['pre_score']\n",
    "    \n",
    "    scores_data[group] = scores_df\n",
    "    print(f\"Group {group}: {len(scores_df)}/{sum(1 for s in stu_const.ALL_STUDENTS if s[stu_const.GROUP_ID] == group)} students with both pre and post tests\")\n",
    "    \n",
    "    # Export\n",
    "    scores_df.to_excel(f\"../debug/debug_learning_gain_{group}.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### 4.2/ Intra-group learning gain analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Do students demonstrate learning gains using Pyrates in their respective groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "groups = [tes_const.GROUP_A, tes_const.GROUP_B, tes_const.GROUP_C]\n",
    "for group in groups:\n",
    "    scores_df = scores_data[group]\n",
    "    pre_scores = scores_df['pre_score']\n",
    "    post_scores = scores_df['post_score']\n",
    "    learning_gains = scores_df['learning_gain']\n",
    "    print(f\"\\n=======================================\")\n",
    "    print(f\"GROUP {group} INTRA-GROUP ANALYSIS\")\n",
    "    print(f\"========================================\")\n",
    "\n",
    "    print(\"\\n=== DESCRIPTIVE STATISTICS ===\")\n",
    "\n",
    "    print(f\"Pre-test: M = {pre_scores.mean():.2f}, SD = {pre_scores.std():.2f}\")\n",
    "    print(f\"Post-test: M = {post_scores.mean():.2f}, SD = {post_scores.std():.2f}\")\n",
    "    print(f\"Learning gain: M = {learning_gains.mean():.2f}, SD = {learning_gains.std():.2f}\")\n",
    "    \n",
    "    print(f\"\\n=== NORMALITY TESTS (Shapiro-Wilk) ===\")\n",
    "    # Shapiro-Wilk test for pre-post differences\n",
    "    shapiro_gain = stats.shapiro(learning_gains)\n",
    "    print(f\"Shapiro-Wilk test for learning gains: p-value = {shapiro_gain.pvalue:.4f}\")\n",
    "    \n",
    "    # Check normality\n",
    "    if shapiro_gain.pvalue >= alpha:\n",
    "        print(\"Learning gains are normally distributed -> Paired T-test is applicable.\")\n",
    "        normal_distribution = True\n",
    "    else:\n",
    "        print(\"Learning gains are not normally distributed -> Prefer Wilcoxon test.\")\n",
    "        normal_distribution = False\n",
    "    \n",
    "    print(f\"\\n=== TEST RESULTS ===\")\n",
    "    # Paired T-test\n",
    "    t_test = stats.ttest_rel(pre_scores, post_scores)\n",
    "    print(f\"Paired T-test: t = {t_test.statistic:.4f}, p-value = {t_test.pvalue:.4f}\")\n",
    "    \n",
    "    # Wilcoxon signed-rank test\n",
    "    wilcoxon_test = stats.wilcoxon(pre_scores, post_scores)\n",
    "    print(f\"Wilcoxon test: W = {wilcoxon_test.statistic:.4f}, p-value = {wilcoxon_test.pvalue:.4f}\")\n",
    "    \n",
    "    # Choose appropriate test based on normality\n",
    "    if normal_distribution:\n",
    "        recommended_pvalue = t_test.pvalue\n",
    "        test_used = \"Paired T-test\"\n",
    "    else:\n",
    "        recommended_pvalue = wilcoxon_test.pvalue\n",
    "        test_used = \"Wilcoxon test\"\n",
    "\n",
    "    # Calculate effect sizes\n",
    "    n = len(learning_gains)\n",
    "    \n",
    "    # Cohen's d for paired samples (using learning gains)\n",
    "    cohens_d = learning_gains.mean() / learning_gains.std()\n",
    "    \n",
    "    # r effect size for Wilcoxon (using z-score approximation)\n",
    "    z_wilcoxon = stats.norm.ppf(1 - wilcoxon_test.pvalue / 2)\n",
    "    r_wilcoxon = z_wilcoxon / np.sqrt(n)\n",
    "\n",
    "\n",
    "    # Choose appropriate test based on normality\n",
    "    if normal_distribution:\n",
    "        recommended_pvalue = t_test.pvalue\n",
    "        test_used = \"Paired T-test\"\n",
    "        effect_size = cohens_d\n",
    "        effect_label = \"Cohen's d\"\n",
    "        \n",
    "        # Interpret Cohen's d\n",
    "        if abs(cohens_d) < 0.2:\n",
    "            effect_interpretation = \"negligible\"\n",
    "        elif abs(cohens_d) < 0.5:\n",
    "            effect_interpretation = \"small\"\n",
    "        elif abs(cohens_d) < 0.8:\n",
    "            effect_interpretation = \"medium\"\n",
    "        else:\n",
    "            effect_interpretation = \"large\"\n",
    "    else:\n",
    "        recommended_pvalue = wilcoxon_test.pvalue\n",
    "        test_used = \"Wilcoxon test\"\n",
    "        effect_size = r_wilcoxon\n",
    "        effect_label = \"r\"\n",
    "        \n",
    "        # Interpret r\n",
    "        if abs(r_wilcoxon) < 0.1:\n",
    "            effect_interpretation = \"negligible\"\n",
    "        elif abs(r_wilcoxon) < 0.3:\n",
    "            effect_interpretation = \"small\"\n",
    "        elif abs(r_wilcoxon) < 0.5:\n",
    "            effect_interpretation = \"medium\"\n",
    "        else:\n",
    "            effect_interpretation = \"large\"\n",
    "    \n",
    "    # Interpretation\n",
    "    print(f\"Results (using {test_used}) :\")\n",
    "    \n",
    "    if recommended_pvalue < alpha:\n",
    "        print(f\"SIGNIFICANT: p-value = {recommended_pvalue:.4f} < alpha = {alpha}\")\n",
    "        if learning_gains.mean() > 0:\n",
    "            print(f\"   Significant learning gain observed (+{learning_gains.mean():.2f} points)\")\n",
    "        else:\n",
    "            print(f\"   Significant learning loss observed ({learning_gains.mean():.2f} points)\")\n",
    "    else:\n",
    "        print(f\"NOT SIGNIFICANT: p-value = {recommended_pvalue:.4f} > alpha = {alpha}\")\n",
    "        print(\"   No significant learning gain observed\")\n",
    "\n",
    "    # Display effect size\n",
    "    print(f\"Effect Size\")\n",
    "    print(f\"{effect_label} = {effect_size:.4f}\")\n",
    "    print(f\"Effect size interpretation: {effect_interpretation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 4.3/ Inter-group learning gain comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### 4.3.1/ Test A VS (B+C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Is there a difference in learning gains between students from group A and those from group B+C ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learning gains\n",
    "learning_gains_A = scores_data['A']['learning_gain']\n",
    "learning_gains_BC = pd.concat([scores_data['B']['learning_gain'], scores_data['C']['learning_gain']])\n",
    "\n",
    "# Descriptive statistics\n",
    "mean_A = learning_gains_A.mean()\n",
    "mean_BC = learning_gains_BC.mean()\n",
    "std_A = learning_gains_A.std()\n",
    "std_BC = learning_gains_BC.std()\n",
    "diff_A_BC = mean_A - mean_BC\n",
    "percent_reduction_BC = (diff_A_BC / mean_A) * 100\n",
    "\n",
    "print(\"=== DESCRIPTIVE STATISTICS ===\")\n",
    "print(f\"Group A (n={len(learning_gains_A)}): M = {mean_A:.2f}, SD = {std_A:.2f}\")\n",
    "print(f\"Group B+C (n={len(learning_gains_BC)}): M = {mean_BC:.2f}, SD = {std_BC:.2f}\")\n",
    "print(f\".   Difference (A - B+C) = {diff_A_BC:.2f}\")\n",
    "print(f\".   Percentage reduction compared to A = {percent_reduction_BC:.2f}%\")\n",
    "\n",
    "alpha = 0.05\n",
    "# Normality test\n",
    "print(\"\\n=== NORMALITY TESTS (Shapiro-Wilk) ===\")\n",
    "shapiro_A = stats.shapiro(learning_gains_A)\n",
    "shapiro_BC = stats.shapiro(learning_gains_BC)\n",
    "print(f\"Group A: p-value = {shapiro_A.pvalue:.4f} ->{'normally distributed' if shapiro_A.pvalue >= alpha else 'not normally distributed'}\")\n",
    "print(f\"Group B+C: p-value = {shapiro_BC.pvalue:.4f} -> {'normally distributed' if shapiro_BC.pvalue >= alpha else 'not normally distributed'}\")\n",
    "\n",
    "# Levene test for equal variances\n",
    "print(f\"\\n=== HOMOGENEITY OF VARIANCES (Levene) ===\")\n",
    "levene_p = stats.levene(learning_gains_A, learning_gains_BC).pvalue\n",
    "print(f\"Levene p-value = {levene_p:.4f} -> {'variances are similar' if levene_p >= alpha else 'variances differ'}\")\n",
    "\n",
    "\n",
    "print(f\"\\n=== TEST RESULTS ===\")\n",
    "# Choose statistical test\n",
    "normal_A = shapiro_A.pvalue >= alpha\n",
    "normal_BC = shapiro_BC.pvalue >= alpha\n",
    "equal_var = levene_p >= alpha\n",
    "\n",
    "if normal_A and normal_BC and equal_var:\n",
    "    print(\"Using independent t-test (parametric)\")\n",
    "    stat, p_value = stats.ttest_ind(learning_gains_A, learning_gains_BC, equal_var=True)\n",
    "    \n",
    "    # Calculate Cohen's d\n",
    "    n1, n2 = len(learning_gains_A), len(learning_gains_BC)\n",
    "    pooled_std = np.sqrt(((n1 - 1) * std_A**2 + (n2 - 1) * std_BC**2) / (n1 + n2 - 2))\n",
    "    cohens_d = (mean_A - mean_BC) / pooled_std\n",
    "\n",
    "    print(f\"Test statistic (t) = {stat:.4f}\")\n",
    "    print(f\"P-value = {p_value:.4f}\")\n",
    "    print(f\"Cohen's d = {cohens_d:.4f}\")\n",
    "    \n",
    "    # Interpret effect size\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect_label = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect_label = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect_label = \"medium\"\n",
    "    else:\n",
    "        effect_label = \"large\"\n",
    "    print(f\"Effect size: {effect_label}\")\n",
    "\n",
    "else:\n",
    "    print(\"Using Mann-Whitney U test (non-parametric)\")\n",
    "    stat, p_value = stats.mannwhitneyu(learning_gains_A, learning_gains_BC, alternative='two-sided')\n",
    "     # Effect size r = Z / sqrt(N)\n",
    "    n_total = len(learning_gains_A) + len(learning_gains_BC)\n",
    "    \n",
    "    # Convert p-value to Z-score (two-sided)\n",
    "    z_score = stats.norm.ppf(1 - p_value / 2)\n",
    "    \n",
    "    r_effect = abs(z_score) / np.sqrt(n_total)\n",
    "\n",
    "    print(f\"Test statistic (U) = {stat:.4f}\")\n",
    "    print(f\"P-value = {p_value:.4f}\")\n",
    "    print(f\"Effect size (r) = {r_effect:.4f}\")\n",
    "\n",
    "    # Interpret effect size\n",
    "    if r_effect < 0.1:\n",
    "        effect_label = \"negligible\"\n",
    "    elif r_effect < 0.3:\n",
    "        effect_label = \"small\"\n",
    "    elif r_effect < 0.5:\n",
    "        effect_label = \"medium\"\n",
    "    else:\n",
    "        effect_label = \"large\"\n",
    "    print(f\"Effect size: {effect_label}\")\n",
    "\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(f\"SIGNIFICANT difference: p = {p_value:.4f} < {alpha}\")\n",
    "else:\n",
    "    print(f\"No significant difference: p = {p_value:.4f} >= {alpha}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "#### 4.3.2/ Test A VS B VS C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Is there a difference in learning gains between students from groups A, B, and C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learning gains for each group\n",
    "learning_gains_A = scores_data['A']['learning_gain']\n",
    "learning_gains_B = scores_data['B']['learning_gain'] \n",
    "learning_gains_C = scores_data['C']['learning_gain']\n",
    "\n",
    "# Descriptive statistics\n",
    "average_gain_A = learning_gains_A.mean()\n",
    "average_gain_B = learning_gains_B.mean()\n",
    "average_gain_C = learning_gains_C.mean()\n",
    "diff_B_vs_A = average_gain_A - average_gain_B\n",
    "diff_C_vs_A = average_gain_A - average_gain_C\n",
    "\n",
    "percent_reduction_B = (diff_B_vs_A / average_gain_A) * 100\n",
    "percent_reduction_C = (diff_C_vs_A / average_gain_A) * 100\n",
    "\n",
    "print(\"\\n=== DESCRIPTIVE STATISTICS ===\")\n",
    "print(f\"Group A (n={len(learning_gains_A)}): M = {average_gain_A:.2f}, SD = {learning_gains_A.std():.2f}\")\n",
    "print(f\"Group B (n={len(learning_gains_B)}): M = {average_gain_B:.2f}, SD = {learning_gains_B.std():.2f}\")\n",
    "print(f\".   B vs A: Mean difference = {diff_B_vs_A:.2f}, Percentage reduction = {percent_reduction_B:.2f}%\")\n",
    "print(f\"Group C (n={len(learning_gains_C)}): M = {average_gain_C:.2f}, SD = {learning_gains_C.std():.2f}\")\n",
    "print(f\".   C vs A: Mean difference = {diff_C_vs_A:.2f}, Percentage reduction = {percent_reduction_C:.2f}%\")\n",
    "\n",
    "alpha = 0.05\n",
    "# Shapiro-Wilk normality test\n",
    "print(f\"\\n=== NORMALITY TESTS (Shapiro-Wilk) ===\")\n",
    "shapiro_A = stats.shapiro(learning_gains_A)\n",
    "shapiro_B = stats.shapiro(learning_gains_B)\n",
    "shapiro_C = stats.shapiro(learning_gains_C)\n",
    "\n",
    "print(f\"Group A: p-value = {shapiro_A.pvalue:.4f}\")\n",
    "print(f\"Group B: p-value = {shapiro_B.pvalue:.4f}\")\n",
    "print(f\"Group C: p-value = {shapiro_C.pvalue:.4f}\")\n",
    "\n",
    "# Check if all groups are normally distributed\n",
    "all_normal = all(p >= alpha for p in [shapiro_A.pvalue, shapiro_B.pvalue, shapiro_C.pvalue])\n",
    "\n",
    "if all_normal:\n",
    "    print(\"All groups follow normal distribution -> ANOVA is applicable.\")\n",
    "else:\n",
    "    print(\"At least one group is not normally distributed -> Prefer Kruskal-Wallis test.\")\n",
    "\n",
    "print(f\"\\n=== HOMOGENEITY OF VARIANCES (Levene) ===\")\n",
    "levene_test = stats.levene(learning_gains_A, learning_gains_B, learning_gains_C)\n",
    "print(f\"Levene's test for homogeneity of variances: p-value = {levene_test.pvalue:.4f}\")\n",
    "\n",
    "if levene_test.pvalue >= alpha:\n",
    "    print(\"Variances are homogeneous\")\n",
    "else:\n",
    "    print(\"Variances are not homogeneous\")\n",
    "\n",
    "# Prepare data for overall tests\n",
    "all_gains = pd.concat([\n",
    "    pd.DataFrame({'group': 'A', 'gain': learning_gains_A}),\n",
    "    pd.DataFrame({'group': 'B', 'gain': learning_gains_B}),\n",
    "    pd.DataFrame({'group': 'C', 'gain': learning_gains_C})\n",
    "])\n",
    "\n",
    "# ANOVA (parametric)\n",
    "anova_result = stats.f_oneway(learning_gains_A, learning_gains_B, learning_gains_C)\n",
    "print(f\"\\n=== ONE-WAY ANOVA RESULTS ===\")\n",
    "print(f\"F-statistic: {anova_result.statistic:.4f}\")\n",
    "print(f\"P-value: {anova_result.pvalue:.4f}\")\n",
    "\n",
    "# Kruskal-Wallis test (non-parametric)\n",
    "kruskal_result = stats.kruskal(learning_gains_A, learning_gains_B, learning_gains_C)\n",
    "print(f\"\\n=== KRUSKAL-WALLIS TEST RESULTS ===\")\n",
    "print(f\"H-statistic: {kruskal_result.statistic:.4f}\")\n",
    "print(f\"P-value: {kruskal_result.pvalue:.4f}\")\n",
    "\n",
    "# Interpretation of overall test\n",
    "if all_normal and levene_test.pvalue >= alpha:\n",
    "    overall_pvalue = anova_result.pvalue\n",
    "    test_used = \"ANOVA\"\n",
    "else:\n",
    "    overall_pvalue = kruskal_result.pvalue\n",
    "    test_used = \"Kruskal-Wallis\"\n",
    "\n",
    "print(f\"\\n=== OVERALL INTERPRETATION (using {test_used}) ===\")\n",
    "if overall_pvalue < alpha:\n",
    "    print(f\"SIGNIFICANT: p-value = {overall_pvalue:.4f} < alpha = {alpha}\")\n",
    "    print(\"   There are statistically significant differences in learning gains between at least two groups\")\n",
    "else:\n",
    "    print(f\"NOT SIGNIFICANT: p-value = {overall_pvalue:.4f} > alpha = {alpha}\")\n",
    "    print(\"   No statistically significant differences in learning gains between groups\")\n",
    "\n",
    "# POST-HOC TESTS (if significant overall difference)\n",
    "if overall_pvalue < alpha:\n",
    "    print(f\"\\n=== POST-HOC PAIRWISE COMPARISONS ===\")\n",
    "    \n",
    "    # For parametric data: Tukey HSD\n",
    "    if test_used == \"ANOVA\":\n",
    "        print(\"Tukey HSD Post-hoc Test:\")\n",
    "        tukey = pairwise_tukeyhsd(endog=all_gains['gain'], groups=all_gains['group'], alpha=alpha)\n",
    "        print(tukey)\n",
    "        \n",
    "        # Extract significant pairs\n",
    "        print(\"\\nSignificant pairwise differences:\")\n",
    "        for i in range(len(tukey._results[0])):\n",
    "            if tukey._results[4][i]:  # If reject (True)\n",
    "                group1, group2 = tukey._results[1][i], tukey._results[2][i]\n",
    "                meandiff = tukey._results[3][i]\n",
    "                pval = tukey._results[4][i]\n",
    "                print(f\"  {group1} vs {group2}: SIGNIFICANT (mean diff = {meandiff:.2f}, p = {pval:.4f})\")\n",
    "    \n",
    "    # For non-parametric data: Mann-Whitney U tests with Bonferroni correction\n",
    "    else:\n",
    "        print(\"Mann-Whitney U tests with Bonferroni correction:\")\n",
    "        pairs = [('A', 'B'), ('A', 'C'), ('B', 'C')]\n",
    "        bonferroni_alpha = alpha / len(pairs)\n",
    "        \n",
    "        for pair in pairs:\n",
    "            group1, group2 = pair\n",
    "            data1 = learning_gains_A if group1 == 'A' else (learning_gains_B if group1 == 'B' else learning_gains_C)\n",
    "            data2 = learning_gains_A if group2 == 'A' else (learning_gains_B if group2 == 'B' else learning_gains_C)\n",
    "            \n",
    "            u_stat, p_value = stats.mannwhitneyu(data1, data2)\n",
    "            adjusted_p = p_value * len(pairs)\n",
    "            significance = \"SIGNIFICANT\" if adjusted_p < alpha else \"not significant\"\n",
    "            \n",
    "            print(f\"  {group1} vs {group2}: p = {p_value:.4f}, adjusted p = {adjusted_p:.4f} -> {significance}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 5/ Teacher solicitation (Q1.3) [A/B/C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### 5.1/ Teacher solicitation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all groups in one go\n",
    "groups = [int_const.GROUP_A, int_const.GROUP_B, int_const.GROUP_C]\n",
    "calls_data = {}\n",
    "\n",
    "for group in groups:\n",
    "    # Filter and count teacher calls\n",
    "    group_data = interaction_data[interaction_data[int_const.GROUP_ID_DATA_KEY] == group]\n",
    "    \n",
    "    teacher_calls = group_data[\n",
    "        (group_data[int_const.ACTION_DATA_KEY] == int_const.RECEIVED_ACTION) &\n",
    "        (group_data[int_const.OBJECT_DATA_KEY] == int_const.TEACHER_HELP_OBJECT)\n",
    "    ]\n",
    "    \n",
    "    # Count per student\n",
    "    calls_per_student = teacher_calls.groupby(int_const.GAME_ID_DATA_KEY).size()\n",
    "    \n",
    "    # Get valid students and fill missing with 0\n",
    "    valid_students = [s[stu_const.GAME_ID] for s in stu_const.ALL_STUDENTS if s[stu_const.GROUP_ID] == group]\n",
    "    calls_per_student = calls_per_student.reindex(valid_students, fill_value=0)\n",
    "    \n",
    "    calls_data[group] = calls_per_student\n",
    "    print(f\"Group {group}: {len(calls_per_student)} students processed\")\n",
    "    \n",
    "    # Export\n",
    "    calls_per_student.to_excel(f\"../debug/debug_teacher_solicitation_per_student_{group}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### 5.2/ Test A VS (B+C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Is there a difference in teacher solicitation between students from group A and students from group B+C ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract calls data for each group\n",
    "calls_A = calls_data['A']\n",
    "calls_B = calls_data['B']\n",
    "calls_C = calls_data['C']\n",
    "\n",
    "# Combine B and C groups\n",
    "calls_BC = pd.concat([calls_B, calls_C])\n",
    "\n",
    "# Descriptive statistics\n",
    "mean_A = calls_A.mean()\n",
    "mean_BC = calls_BC.mean()\n",
    "std_A = calls_A.std()\n",
    "std_BC = calls_BC.std()\n",
    "diff_A_BC = mean_A - mean_BC\n",
    "percent_reduction_BC = (diff_A_BC / mean_A) * 100\n",
    "\n",
    "print(\"\\n=== DESCRIPTIVE STATISTICS ===\")\n",
    "print(f\"Group A (n={len(calls_A)}): M = {mean_A:.2f}, SD = {std_A:.2f}\")\n",
    "print(f\"Group B+C (n={len(calls_BC)}): M = {mean_BC:.2f}, SD = {std_BC:.2f}\")\n",
    "print(f\".   Difference (A - B+C) = {diff_A_BC:.2f}\")\n",
    "print(f\".   Percentage reduction compared to A = {percent_reduction_BC:.2f}%\")\n",
    "\n",
    "alpha = 0.05\n",
    "# Shapiro-Wilk normality test\n",
    "print(\"\\n=== NORMALITY TESTS (Shapiro-Wilk) ===\")\n",
    "shapiro_A = stats.shapiro(calls_A)\n",
    "shapiro_BC = stats.shapiro(calls_BC)\n",
    "print(f\"Group A: p-value = {shapiro_A.pvalue:.4f} -> {'normally distributed' if shapiro_A.pvalue >= alpha else 'not normally distributed'}\")\n",
    "print(f\"Group B+C: p-value = {shapiro_BC.pvalue:.4f} -> {'normally distributed' if shapiro_BC.pvalue >= alpha else 'not normally distributed'}\")\n",
    "\n",
    "# Levene test for equal variances\n",
    "print(f\"\\n=== HOMOGENEITY OF VARIANCES (Levene) ===\")\n",
    "levene_p = stats.levene(calls_A, calls_BC).pvalue\n",
    "print(f\"Levene p-value = {levene_p:.4f} -> {'variances are similar' if levene_p >= alpha else 'variances differ'}\")\n",
    "\n",
    "print(f\"\\n=== TEST RESULTS ===\")\n",
    "# Choose statistical test\n",
    "normal_A = shapiro_A.pvalue >= alpha\n",
    "normal_BC = shapiro_BC.pvalue >= alpha\n",
    "equal_var = levene_p >= alpha\n",
    "\n",
    "if normal_A and normal_BC and equal_var:\n",
    "    print(\"Using independent t-test (parametric)\")\n",
    "    stat, p_value = stats.ttest_ind(calls_A, calls_BC, equal_var=True)\n",
    "    # Cohen's d\n",
    "    n1, n2 = len(calls_A), len(calls_BC)\n",
    "    pooled_std = np.sqrt(((n1 - 1)*std_A**2 + (n2 - 1)*std_BC**2)/(n1 + n2 - 2))\n",
    "    cohens_d = diff_A_BC / pooled_std\n",
    "    print(f\"Test statistic (t) = {stat:.4f}\")\n",
    "    print(f\"P-value = {p_value:.4f}\")\n",
    "    print(f\"Cohen's d = {cohens_d:.4f}\")\n",
    "    \n",
    "    # Interpret effect size\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect_label = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect_label = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect_label = \"medium\"\n",
    "    else:\n",
    "        effect_label = \"large\"\n",
    "    print(f\"Effect size: {effect_label}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Using Mann-Whitney U test (non-parametric)\")\n",
    "    stat, p_value = stats.mannwhitneyu(calls_A, calls_BC, alternative='two-sided')\n",
    "    # Effect size r = Z / sqrt(N)\n",
    "    n_total = len(calls_A) + len(calls_BC)\n",
    "    z_score = stats.norm.ppf(1 - p_value / 2)  # two-sided\n",
    "    r_effect = abs(z_score) / np.sqrt(n_total)\n",
    "    print(f\"Test statistic (U) = {stat:.4f}\")\n",
    "    print(f\"P-value = {p_value:.4f}\")\n",
    "    print(f\"Effect size (r) = {r_effect:.4f}\")\n",
    "\n",
    "    # Interpret effect size\n",
    "    if r_effect < 0.1:\n",
    "        effect_label = \"negligible\"\n",
    "    elif r_effect < 0.3:\n",
    "        effect_label = \"small\"\n",
    "    elif r_effect < 0.5:\n",
    "        effect_label = \"medium\"\n",
    "    else:\n",
    "        effect_label = \"large\"\n",
    "    print(f\"Effect size: {effect_label}\")\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(f\"SIGNIFICANT difference: p = {p_value:.4f} < {alpha}\")\n",
    "    print(\"There are statistically significant differences in teacher solicitation between Group A and Group B+C\")\n",
    "else:\n",
    "    print(f\"No significant difference: p = {p_value:.4f} >= {alpha}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### 5.3/ Test A VS B VS C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Is there a difference in teacher solicitation between students from groups A, B, and C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "average_calls_A = calls_A.mean()\n",
    "average_calls_B = calls_B.mean()\n",
    "average_calls_C = calls_C.mean()\n",
    "\n",
    "diff_B_A = average_calls_A - average_calls_B\n",
    "diff_C_A = average_calls_A - average_calls_C\n",
    "\n",
    "percent_reduction_B = (diff_B_A / average_calls_A) * 100\n",
    "percent_reduction_C = (diff_C_A / average_calls_A) * 100\n",
    "\n",
    "print(\"\\n=== DESCRIPTIVE STATISTICS ===\")\n",
    "print(f\"Group A (n={len(calls_A)}): M = {average_calls_A:.2f}, SD = {calls_A.std():.2f}\")\n",
    "print(f\"Group B (n={len(calls_B)}): M = {average_calls_B:.2f}, SD = {calls_B.std():.2f}\")\n",
    "print(f\".   B vs A: Difference = {diff_B_A:.2f}, Percentage reduction = {percent_reduction_B:.2f}%\")\n",
    "print(f\"Group C (n={len(calls_C)}): M = {average_calls_C:.2f}, SD = {calls_C.std():.2f}\")\n",
    "print(f\".   C vs A: Difference = {diff_C_A:.2f}, Percentage reduction = {percent_reduction_C:.2f}%\")\n",
    "\n",
    "alpha = 0.05\n",
    "# Shapiro-Wilk normality test\n",
    "print(f\"\\n=== NORMALITY TESTS (Shapiro-Wilk) ===\")\n",
    "shapiro_A = stats.shapiro(calls_A)\n",
    "shapiro_B = stats.shapiro(calls_B)\n",
    "shapiro_C = stats.shapiro(calls_C)\n",
    "print(f\"Group A: p-value = {shapiro_A.pvalue:.4f}\")\n",
    "print(f\"Group B: p-value = {shapiro_B.pvalue:.4f}\")\n",
    "print(f\"Group C: p-value = {shapiro_C.pvalue:.4f}\")\n",
    "\n",
    "# Check if all groups are normally distributed\n",
    "all_normal = all(p >= alpha for p in [shapiro_A.pvalue, shapiro_B.pvalue, shapiro_C.pvalue])\n",
    "\n",
    "if all_normal:\n",
    "    print(\"All groups follow normal distribution -> ANOVA is applicable.\")\n",
    "else:\n",
    "    print(\"At least one group is not normally distributed -> Prefer Kruskal-Wallis test.\")\n",
    "\n",
    "# Test for homogeneity of variances (Levene's test)\n",
    "print(f\"\\n=== HOMOGENEITY OF VARIANCES (Levene) ===\")\n",
    "\n",
    "levene_test = stats.levene(calls_A, calls_B, calls_C)\n",
    "print(f\"Levene's test for homogeneity of variances: p-value = {levene_test.pvalue:.4f}\")\n",
    "\n",
    "if levene_test.pvalue >= alpha:\n",
    "    print(\"Variances are homogeneous\")\n",
    "else:\n",
    "    print(\"Variances are not homogeneous\")\n",
    "\n",
    "# Prepare data for overall tests\n",
    "all_calls = pd.concat([\n",
    "    pd.DataFrame({'group': 'A', 'calls': calls_A}),\n",
    "    pd.DataFrame({'group': 'B', 'calls': calls_B}),\n",
    "    pd.DataFrame({'group': 'C', 'calls': calls_C})\n",
    "])\n",
    "\n",
    "# ANOVA (parametric)\n",
    "anova_result = stats.f_oneway(calls_A, calls_B, calls_C)\n",
    "print(f\"\\n=== ONE-WAY ANOVA RESULTS ===\")\n",
    "print(f\"F-statistic: {anova_result.statistic:.4f}\")\n",
    "print(f\"P-value: {anova_result.pvalue:.4f}\")\n",
    "\n",
    "# Kruskal-Wallis test (non-parametric)\n",
    "kruskal_result = stats.kruskal(calls_A, calls_B, calls_C)\n",
    "print(f\"\\n=== KRUSKAL-WALLIS TEST RESULTS ===\")\n",
    "print(f\"H-statistic: {kruskal_result.statistic:.4f}\")\n",
    "print(f\"P-value: {kruskal_result.pvalue:.4f}\")\n",
    "\n",
    "# Interpretation of overall test\n",
    "if all_normal and levene_test.pvalue >= alpha:\n",
    "    overall_pvalue = anova_result.pvalue\n",
    "    test_used = \"ANOVA\"\n",
    "else:\n",
    "    overall_pvalue = kruskal_result.pvalue\n",
    "    test_used = \"Kruskal-Wallis\"\n",
    "\n",
    "print(f\"\\n=== OVERALL INTERPRETATION (using {test_used}) ===\")\n",
    "if overall_pvalue < alpha:\n",
    "    print(f\"SIGNIFICANT: p-value = {overall_pvalue:.4f} < alpha = {alpha}\")\n",
    "    print(\"   There are statistically significant differences in teacher solicitation between at least two groups\")\n",
    "else:\n",
    "    print(f\"NOT SIGNIFICANT: p-value = {overall_pvalue:.4f} > alpha = {alpha}\")\n",
    "    print(\"   No statistically significant differences in teacher solicitation between groups\")\n",
    "\n",
    "# POST-HOC TESTS (if significant overall difference)\n",
    "if overall_pvalue < alpha:\n",
    "    print(f\"\\n=== POST-HOC PAIRWISE COMPARISONS ===\")\n",
    "    \n",
    "    # For parametric data: Tukey HSD\n",
    "    if test_used == \"ANOVA\":\n",
    "        print(\"Tukey HSD Post-hoc Test:\")\n",
    "        tukey = pairwise_tukeyhsd(endog=all_calls['calls'], groups=all_calls['group'], alpha=alpha)\n",
    "        print(tukey)\n",
    "        \n",
    "    # For non-parametric data: Mann-Whitney U tests with Bonferroni correction\n",
    "    else:\n",
    "        print(\"Mann-Whitney U tests with Bonferroni correction:\")\n",
    "        pairs = [('A', 'B'), ('A', 'C'), ('B', 'C')]\n",
    "        bonferroni_alpha = alpha / len(pairs)\n",
    "        \n",
    "        for pair in pairs:\n",
    "            group1, group2 = pair\n",
    "            data1 = calls_A if group1 == 'A' else (calls_B if group1 == 'B' else calls_C)\n",
    "            data2 = calls_A if group2 == 'A' else (calls_B if group2 == 'B' else calls_C)\n",
    "            \n",
    "            u_stat, p_value = stats.mannwhitneyu(data1, data2)\n",
    "            adjusted_p = p_value * len(pairs)\n",
    "            significance = \"SIGNIFICANT\" if adjusted_p < alpha else \"not significant\"\n",
    "            \n",
    "            print(f\"  {group1} vs {group2}: p = {p_value:.4f}, adjusted p = {adjusted_p:.4f} -> {significance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## 6/ Activity following feedback (Q1.4) [B/C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### 6.1/ Calculation of student activity following feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "1/ Following a request to the digital assistant, we categorize the subsequent sequence into four categories:\n",
    "- `progression`: the student managed to progress in the game (first increment of game_progression or reach of next level) without calling the teacher or the digital assistant again\n",
    "- `teacher_call`: the student requested help from the teacher (without having progressed in the game)\n",
    "- `assistant_call`: the student requested help from the digital assistant again (without having progressed in the game)\n",
    "- `other`: other cases, for example, the experiment ends\n",
    "\n",
    "Note that we do not have in the traces the start date of teacher interventions but only the end date. This is why, when we detect a progression in a sequence, we search in the following 120 seconds for a trace of the end of a teacher intervention. If we find one, we reclassify the sequence as `teacher_call`.\n",
    "\n",
    "2/ Moreover, when a progression sequence occurs, we compute:\n",
    "- the progression gain (difference in game_progression)\n",
    "- the sequence of actions before the progression occurred (see `categorize_action()` function bellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for groups B and C\n",
    "groups_BC_interaction = interaction_data[\n",
    "    interaction_data[int_const.GROUP_ID_DATA_KEY].isin([int_const.GROUP_B, int_const.GROUP_C])\n",
    "]\n",
    "\n",
    "# Sort by student and date to analyze sequences\n",
    "groups_BC_interaction = groups_BC_interaction.sort_values([\n",
    "    int_const.GAME_ID_DATA_KEY, \n",
    "    int_const.DATE_DATA_KEY\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# Filter all feedback reception events\n",
    "feedback_events = groups_BC_interaction[\n",
    "    (groups_BC_interaction[int_const.ACTION_DATA_KEY] == int_const.RECEIVED_ACTION) &\n",
    "    (groups_BC_interaction[int_const.OBJECT_DATA_KEY] == int_const.ASSISTANT_HELP_OBJECT)\n",
    "].copy()\n",
    "\n",
    "\n",
    "# Define action categories\n",
    "def categorize_action(row):\n",
    "    action = row[int_const.ACTION_DATA_KEY]\n",
    "    object_name = row[int_const.OBJECT_DATA_KEY]\n",
    "    # Display of the memo\n",
    "    if action == int_const.DISPLAYED_ACTION:\n",
    "        return \"memo_displayed\"\n",
    "    # Copy of feedback content\n",
    "    elif (action == int_const.COPIED_ACTION and \n",
    "          object_name == int_const.ASSISTANT_MESSAGES_SECTION_OBJECT):\n",
    "        return \"feedback_content_copied\"\n",
    "    # Copy of the memo content\n",
    "    elif (action == int_const.COPIED_ACTION and \n",
    "          object_name in [\n",
    "              int_const.BASE_PROGRAM_SECTION_OBJECT,\n",
    "              int_const.BASE_ERROR_SECTION_OBJECT,\n",
    "              int_const.BASE_STRUCTURE_SECTION_OBJECT,\n",
    "              int_const.BASE_COMMENT_SECTION_OBJECT,\n",
    "              int_const.VAR_CREATION_SECTION_OBJECT,\n",
    "              int_const.VAR_MODIFICATION_SECTION_OBJECT,\n",
    "              int_const.VAR_USAGE_SECTION_OBJECT,\n",
    "              int_const.VAR_TYPE_SECTION_OBJECT,\n",
    "              int_const.CONDI_1BRAN_SECTION_OBJECT,\n",
    "              int_const.CONDI_2BRAN_SECTION_OBJECT,\n",
    "              int_const.CONDI_3BRAN_SECTION_OBJECT,\n",
    "              int_const.FOR_SIMPLE_SECTION_OBJECT,\n",
    "              int_const.FOR_COUNTER_0_SECTION_OBJECT,\n",
    "              int_const.FOR_COUNTER_N_SECTION_OBJECT,\n",
    "              int_const.WHILE_SIMPLE_SECTION_OBJECT\n",
    "          ]):\n",
    "        return \"memo_content_copied\"\n",
    "    # Copy of code editor\n",
    "    elif (action == int_const.COPIED_ACTION and \n",
    "          object_name == int_const.CODE_EDITOR_SECTION_OBJECT):\n",
    "        return \"code_editor_copied\"\n",
    "    # Copy of control function\n",
    "    elif (action == int_const.COPIED_ACTION and \n",
    "          object_name == int_const.CONTROL_FUNCTION_SECTION_OBJECT):\n",
    "        return \"control_function_copied\"\n",
    "    # Code paste\n",
    "    elif action == int_const.PASTED_ACTION:\n",
    "        return \"code_pasted\"\n",
    "    # Launch of a correct program\n",
    "    elif (action == int_const.LAUNCHED_ACTION and \n",
    "          object_name in [\n",
    "              int_const.FULLY_EXECUTED_PROGRAM_OBJECT,\n",
    "              int_const.LEVEL_COMPLETED_PROGRAM_OBJECT\n",
    "          ]):\n",
    "        return \"correct_program_launched\"\n",
    "    # Launch of an erroneous program\n",
    "    elif (action == int_const.LAUNCHED_ACTION and \n",
    "          object_name in [\n",
    "              int_const.LEVEL_LOST_PROGRAM_OBJECT,\n",
    "              int_const.TOO_MANY_LINES_ERROR_PROGRAM_OBJECT,\n",
    "              int_const.GAME_ERROR_PROGRAM_OBJECT,\n",
    "              int_const.SYNTACTIC_ERROR_PROGRAM_OBJECT,\n",
    "              int_const.SEMANTIC_ERROR_PROGRAM_OBJECT\n",
    "          ]):\n",
    "        return \"erroneous_program_launched\"\n",
    "    # Launch of a stopped program\n",
    "    elif (action == int_const.LAUNCHED_ACTION and \n",
    "          object_name == int_const.USER_STOPPED_PROGRAM_OBJECT):\n",
    "        return \"stopped_program_launched\"\n",
    "    # Leave to visit a completed level\n",
    "    elif action == int_const.LEAVED_ACTION:\n",
    "        return \"level_leaved\"\n",
    "    # Back from a completed level\n",
    "    elif action in [int_const.RESUMED_ACTION, int_const.RESTARTED_ACTION]:\n",
    "        return \"level_resumed\"\n",
    "    # End of the level (not a real action)\n",
    "    elif action == int_const.ENDED_ACTION:\n",
    "        return \"level_ended\"\n",
    "    # Assistant help called\n",
    "    elif (action == int_const.ASKED_ACTION and \n",
    "          object_name == int_const.ASSISTANT_HELP_OBJECT):\n",
    "        return \"assistant_help_called\"\n",
    "    # Assistant help received\n",
    "    elif (action == int_const.RECEIVED_ACTION and \n",
    "          object_name == int_const.ASSISTANT_HELP_OBJECT):\n",
    "        return \"assistant_help_received\"\n",
    "    # Teacher help received\n",
    "    elif (action == int_const.RECEIVED_ACTION and \n",
    "          object_name == int_const.TEACHER_HELP_OBJECT):\n",
    "        return \"teacher_help_received\"\n",
    "    # Change of speed cursor\n",
    "    elif (action == int_const.CHANGED_ACTION and \n",
    "          object_name == int_const.EXECUTION_SPEED_SLIDE_OBJECT):\n",
    "        return \"speed_cursor_changed\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "\n",
    "feedback_analysis = []\n",
    "# Iterate on each feedback event\n",
    "for _, feedback in feedback_events.iterrows():\n",
    "    game_id = feedback[int_const.GAME_ID_DATA_KEY]\n",
    "    group_id = feedback[int_const.GROUP_ID_DATA_KEY]\n",
    "    feedback_date = feedback[int_const.DATE_DATA_KEY]\n",
    "    feedback_level = feedback[int_const.LEVEL_DATA_KEY]\n",
    "    feedback_progression = feedback[int_const.GAME_PROGRESSION_DATA_KEY]\n",
    "    \n",
    "    # Get student's traces after this feedback\n",
    "    student_traces = groups_BC_interaction[\n",
    "        (groups_BC_interaction[int_const.GAME_ID_DATA_KEY] == game_id) &\n",
    "        (groups_BC_interaction[int_const.DATE_DATA_KEY] > feedback_date)\n",
    "    ]\n",
    "    \n",
    "    actions_after_feedback = []\n",
    "    sequence_outcome = \"other\"  # Default outcome\n",
    "    progression_gain = 0  # Initialize progression gain\n",
    "    progression_date = None  # Track when progression happened\n",
    "    \n",
    "    # Iterate on post-feedback traces\n",
    "    for _, trace in student_traces.iterrows():\n",
    "        current_progression = trace[int_const.GAME_PROGRESSION_DATA_KEY]\n",
    "        current_level = trace[int_const.LEVEL_DATA_KEY]\n",
    "        \n",
    "        # Check for teacher call progression\n",
    "        if (trace[int_const.ACTION_DATA_KEY] == int_const.RECEIVED_ACTION and \n",
    "            trace[int_const.OBJECT_DATA_KEY] == int_const.TEACHER_HELP_OBJECT):\n",
    "            sequence_outcome = \"teacher_call\"\n",
    "            break\n",
    "        \n",
    "        # Check for new assistant call progression\n",
    "        if (trace[int_const.ACTION_DATA_KEY] == int_const.ASKED_ACTION and \n",
    "            trace[int_const.OBJECT_DATA_KEY] == int_const.ASSISTANT_HELP_OBJECT):\n",
    "            sequence_outcome = \"assistant_call\"\n",
    "            break\n",
    "        \n",
    "        # Check if progression increased (in same level) or level increased\n",
    "        progression_increase = (current_level == feedback_level and \n",
    "                               current_progression > feedback_progression)\n",
    "        level_increase = current_level > feedback_level\n",
    "\n",
    "        # Check if progression achieved\n",
    "        if progression_increase or level_increase:\n",
    "            sequence_outcome = \"progression\"\n",
    "            # Calculate progression gain\n",
    "            if level_increase:\n",
    "                progression_gain = 100 - feedback_progression\n",
    "            else:\n",
    "                progression_gain = current_progression - feedback_progression\n",
    "            \n",
    "            # Store the date when progression happened\n",
    "            progression_date = trace[int_const.DATE_DATA_KEY]\n",
    "            \n",
    "            # We break without adding the action that make progression detection\n",
    "            # That's because in the Pyrates trace system, the progression is reported in the next trace\n",
    "            # i.e. each trace report the progression before its action\n",
    "            break\n",
    "        \n",
    "        # Categorize and count all sequence actions\n",
    "        action_type = categorize_action(trace)\n",
    "        # Ignore level_end which is not really an action\n",
    "        if action_type not in [\"level_ended\"]:\n",
    "            actions_after_feedback.append(action_type)\n",
    "    \n",
    "    # Check for teacher call within 120 seconds after detected progression\n",
    "    if sequence_outcome == \"progression\" and progression_date is not None:\n",
    "        # Look for teacher help in the 120 seconds following progression\n",
    "        teacher_help_after_progression = groups_BC_interaction[\n",
    "            (groups_BC_interaction[int_const.GAME_ID_DATA_KEY] == game_id) &\n",
    "            (groups_BC_interaction[int_const.DATE_DATA_KEY] > progression_date) &\n",
    "            (groups_BC_interaction[int_const.DATE_DATA_KEY] <= progression_date + pd.Timedelta(seconds=120)) &\n",
    "            (groups_BC_interaction[int_const.ACTION_DATA_KEY] == int_const.RECEIVED_ACTION) &\n",
    "            (groups_BC_interaction[int_const.OBJECT_DATA_KEY] == int_const.TEACHER_HELP_OBJECT)\n",
    "        ]\n",
    "        \n",
    "        if not teacher_help_after_progression.empty:\n",
    "            sequence_outcome = \"teacher_call\"\n",
    "    \n",
    "    # Store analysis for this feedback event\n",
    "    feedback_analysis.append({\n",
    "        'game_id': game_id,\n",
    "        'group': group_id,\n",
    "        'feedback_level': feedback_level,\n",
    "        'feedback_progression': feedback_progression,\n",
    "        'actions_count': len(actions_after_feedback),\n",
    "        'actions_sequence': actions_after_feedback,\n",
    "        'sequence_outcome': sequence_outcome,\n",
    "        'progression_gain': progression_gain,\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "analysis_df = pd.DataFrame(feedback_analysis)\n",
    "# Export\n",
    "analysis_df.to_excel(f\"../debug/debug_activity_following_feedback.xlsx\")\n",
    "\n",
    "print(f\"Total sequences analyzed: {len(feedback_analysis)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### 6.2/ Feedback outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "What is the outcome of the action sequences following the reception of feedback depending on the groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "groups = [int_const.GROUP_B, int_const.GROUP_C]\n",
    "categories = ['assistant_call', 'progression', 'teacher_call', 'other']\n",
    "\n",
    "\n",
    "# Store counts, totals, and significance\n",
    "counts_dict = {}\n",
    "totals_dict = {}\n",
    "significant_flags = {}\n",
    "\n",
    "# Print feedback outcomes per group\n",
    "for group in groups:\n",
    "    group_data = analysis_df[analysis_df['group'] == group]\n",
    "    \n",
    "    print(f\"\\n=======================================\")\n",
    "    print(f\"GROUP {group} FEEDBACK OUTCOMES\")\n",
    "    print(f\"========================================\")\n",
    "    print(f\"Total feedback sequences: {len(group_data)}\")\n",
    "    \n",
    "    outcome_counts = group_data['sequence_outcome'].value_counts()\n",
    "    counts_dict[group] = np.array([outcome_counts.get(cat, 0) for cat in categories])\n",
    "    totals_dict[group] = len(group_data)\n",
    "    \n",
    "    for outcome in categories:\n",
    "        count = outcome_counts.get(outcome, 0)\n",
    "        percentage = (count / len(group_data)) * 100\n",
    "        print(f\"   {outcome}: {count} sequences ({percentage:.1f}%)\")\n",
    "\n",
    "# Contingency table for Chi-square test\n",
    "contingency_table = np.array([counts_dict[int_const.GROUP_B], counts_dict[int_const.GROUP_C]]).T\n",
    "\n",
    "chi2, p_chi2, dof, expected = chi2_contingency(contingency_table)\n",
    "print(\"\\n=== Chi-square Test (B vs C) ===\")\n",
    "print(f\"Chi2 statistic = {chi2:.2f}, p-value = {p_chi2:.4f}\")\n",
    "\n",
    "# Z-tests for proportions per category (Bonferroni correction)\n",
    "print(\"\\n=== Z-test for proportions by category ===\")\n",
    "n_B = totals_dict[int_const.GROUP_B]\n",
    "n_C = totals_dict[int_const.GROUP_C]\n",
    "num_tests = len(categories)\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    counts_i = np.array([counts_dict[int_const.GROUP_B][i], counts_dict[int_const.GROUP_C][i]])\n",
    "    nobs = np.array([n_B, n_C])\n",
    "    \n",
    "    stat, pval = proportions_ztest(counts_i, nobs)\n",
    "    adjusted_p = min(pval * num_tests, 1.0)  # Bonferroni correction\n",
    "    significance = adjusted_p < alpha\n",
    "    significant_flags[category] = significance \n",
    "    \n",
    "    label = \"SIGNIFICANT\" if significance else \"not significant\"\n",
    "    print(f\"{category}: Z = {stat:.2f}, raw p-value = {pval:.4f}, adjusted p-value = {adjusted_p:.4f} -> {label}\")\n",
    "\n",
    "\n",
    "print(\"\\nOverall interpretation:\")\n",
    "if p_chi2 < alpha:\n",
    "    print(\"The overall distribution of actions following feedback differs significantly between B and C.\")\n",
    "else:\n",
    "    print(\"No significant overall difference between B and C.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_plot = 4 \n",
    "groups = [int_const.GROUP_B, int_const.GROUP_C]\n",
    "\n",
    "# Slice precomputed results\n",
    "display_categories = categories[:top_n_plot]\n",
    "percentage_df_plot = pd.DataFrame(index=display_categories, columns=groups, dtype=float)\n",
    "\n",
    "for cat in display_categories:\n",
    "    for group in groups:\n",
    "        total = totals_dict[group]\n",
    "        count = counts_dict[group][categories.index(cat)]\n",
    "        percentage_df_plot.loc[cat, group] = (count / total) * 100 if total > 0 else 0\n",
    "\n",
    "# X positions\n",
    "x = np.arange(len(display_categories))\n",
    "width = 0.4\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars_B = ax.bar(x - width/2, percentage_df_plot[int_const.GROUP_B], width, label='Group B', color='skyblue')\n",
    "bars_C = ax.bar(x + width/2, percentage_df_plot[int_const.GROUP_C], width, label='Group C', color='salmon')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(display_categories, rotation=30, ha='right')\n",
    "ax.set_xlabel(\"Feedback outcome\", fontsize=12)\n",
    "ax.set_ylabel(\"Percentage (%)\", fontsize=12)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax.set_ylim(0, max(percentage_df_plot.max()) + 5)\n",
    "\n",
    "# Add percentage labels and significance stars\n",
    "for i, cat in enumerate(display_categories):\n",
    "    height_B = percentage_df_plot[int_const.GROUP_B][cat]\n",
    "    height_C = percentage_df_plot[int_const.GROUP_C][cat]\n",
    "    \n",
    "    # Labels on top of each bar\n",
    "    ax.annotate(f'{height_B:.1f}%', xy=(x[i] - width/2, height_B), xytext=(0,3),\n",
    "                textcoords=\"offset points\", ha='center', va='bottom', fontsize=10)\n",
    "    ax.annotate(f'{height_C:.1f}%', xy=(x[i] + width/2, height_C), xytext=(0,3),\n",
    "                textcoords=\"offset points\", ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Add significance star above bars if precomputed as significant\n",
    "    if significant_flags[cat]:\n",
    "        y_star = max(height_B, height_C) + 3\n",
    "        ax.text(x[i], y_star, '*', ha='center', va='bottom', fontsize=16, color='black')\n",
    "\n",
    "# Legend and layout\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/feedback_outcomes_BC.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### 6.3/ Progression gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "What is the progression gain resulting of a progression outcome depending on the groups ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract progression-only data\n",
    "progress_B = analysis_df[(analysis_df['group'] == int_const.GROUP_B) & \n",
    "                         (analysis_df['sequence_outcome'] == 'progression')]\n",
    "\n",
    "progress_C = analysis_df[(analysis_df['group'] == int_const.GROUP_C) & \n",
    "                         (analysis_df['sequence_outcome'] == 'progression')]\n",
    "\n",
    "data_B = progress_B[\"progression_gain\"].dropna()\n",
    "data_C = progress_C[\"progression_gain\"].dropna()\n",
    "\n",
    "# Descriptive statistics\n",
    "mean_B = data_B.mean()\n",
    "mean_C = data_C.mean()\n",
    "std_B = data_B.std()\n",
    "std_C = data_C.std()\n",
    "\n",
    "# Difference in means\n",
    "mean_difference = mean_C - mean_B\n",
    "# Percentage increase (relative to group B)\n",
    "percent_increase = (mean_difference / mean_B) * 100\n",
    "\n",
    "print(\"=== DESCRIPTIVE STATISTICS ===\")\n",
    "print(f\"Group B (n={len(data_B)}): M = {mean_B:.2f}, SD = {std_B:.2f}\")\n",
    "print(f\"Group C (n={len(data_C)}): M = {mean_C:.2f}, SD = {std_C:.2f}\")\n",
    "print(f\".   Difference (C - B): {mean_difference:.2f}\")\n",
    "print(f\".   Percentage increase from B to C: {percent_increase:.2f}%\")\n",
    "\n",
    "alpha = 0.05\n",
    "# Normality test\n",
    "print(\"\\n=== NORMALITY TESTS (Shapiro-Wilk) ===\")\n",
    "shapiro_B = stats.shapiro(data_B)\n",
    "shapiro_C = stats.shapiro(data_C)\n",
    "print(f\"Group B: p-value = {shapiro_B.pvalue:.4f} -> \"\n",
    "        f\"{'normally distributed' if shapiro_B.pvalue >= alpha else 'not normally distributed'}\")\n",
    "print(f\"Group C: p-value = {shapiro_C.pvalue:.4f} -> \"\n",
    "        f\"{'normally distributed' if shapiro_C.pvalue >= alpha else 'not normally distributed'}\")\n",
    "\n",
    "# Levene test\n",
    "print(\"\\n=== HOMOGENEITY OF VARIANCES (Levene) ===\")\n",
    "levene_p = stats.levene(data_B, data_C).pvalue\n",
    "\n",
    "print(f\"Levene p-value = {levene_p:.4f} -> \"\n",
    "        f\"{'variances are similar' if levene_p >= alpha else 'variances differ'}\")\n",
    "\n",
    "print(f\"\\n=== TEST RESULTS ===\")\n",
    "# Choose appropriate test\n",
    "normal_B = shapiro_B.pvalue >= alpha\n",
    "normal_C = shapiro_C.pvalue >= alpha\n",
    "equal_var = levene_p >= alpha\n",
    "\n",
    "if normal_B and normal_C and equal_var:\n",
    "    print(\"Using independent t-test (parametric)\")\n",
    "    stat, p_value = stats.ttest_ind(data_B, data_C, equal_var=True)\n",
    "    # Cohen's d\n",
    "    n1, n2 = len(data_B), len(data_C)\n",
    "    pooled_std = np.sqrt(((n1 - 1) * std_B**2 + (n2 - 1) * std_C**2) / (n1 + n2 - 2))\n",
    "    cohens_d = (mean_C - mean_B) / pooled_std\n",
    "    \n",
    "    print(f\"Test statistic (t) = {stat:.4f}\")\n",
    "    print(f\"P-value = {p_value:.4f}\")\n",
    "    print(f\"Cohen's d = {cohens_d:.4f}\")\n",
    "    \n",
    "    # Interpret effect size\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect_label = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect_label = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect_label = \"medium\"\n",
    "    else:\n",
    "        effect_label = \"large\"\n",
    "    print(f\"Effect size: {effect_label}\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Using Mann-Whitney U test (non-parametric)\")\n",
    "    stat, p_value = stats.mannwhitneyu(data_B, data_C, alternative='two-sided')\n",
    "    # Effect size r = Z / sqrt(N)\n",
    "    n_total = len(data_B) + len(data_C)\n",
    "    # Convert p-value to two-tailed Z-score\n",
    "    z_score = stats.norm.ppf(1 - p_value / 2)\n",
    "    r_effect = abs(z_score) / np.sqrt(n_total)\n",
    "    \n",
    "    print(f\"Test statistic (U) = {stat:.4f}\")\n",
    "    print(f\"P-value = {p_value:.4f}\")\n",
    "    print(f\"Effect size (r) = {r_effect:.4f}\")\n",
    "    \n",
    "    # Interpret effect size\n",
    "    if r_effect < 0.1:\n",
    "        effect_label = \"negligible\"\n",
    "    elif r_effect < 0.3:\n",
    "        effect_label = \"small\"\n",
    "    elif r_effect < 0.5:\n",
    "        effect_label = \"medium\"\n",
    "    else:\n",
    "        effect_label = \"large\"\n",
    "    print(f\"Effect size: {effect_label}\")\n",
    "    \n",
    "\n",
    "if p_value < alpha:\n",
    "    print(f\"SIGNIFICANT difference: p = {p_value:.4f} < {alpha}\")\n",
    "else:\n",
    "    print(f\"No significant difference: p = {p_value:.4f} >= {alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### 6.4/ Number of actions before progression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "How many actions do students need to achieve a progression in the game following feedback depending on the groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract progression-only data\n",
    "progress_B = analysis_df[(analysis_df['group'] == int_const.GROUP_B) & \n",
    "                         (analysis_df['sequence_outcome'] == 'progression')]\n",
    "\n",
    "progress_C = analysis_df[(analysis_df['group'] == int_const.GROUP_C) & \n",
    "                         (analysis_df['sequence_outcome'] == 'progression')]\n",
    "\n",
    "data_B = progress_B[\"actions_count\"].dropna()\n",
    "data_C = progress_C[\"actions_count\"].dropna()\n",
    "\n",
    "# Descriptive statistics\n",
    "mean_B = data_B.mean()\n",
    "mean_C = data_C.mean()\n",
    "std_B = data_B.std()\n",
    "std_C = data_C.std()\n",
    "\n",
    "# Difference in means\n",
    "mean_difference = mean_C - mean_B\n",
    "# Percentage increase (relative to group B)\n",
    "percent_increase = (mean_difference / mean_B) * 100\n",
    "\n",
    "print(\"=== DESCRIPTIVE STATISTICS ===\")\n",
    "print(f\"Group B (n={len(data_B)}): M = {mean_B:.2f}, SD = {std_B:.2f}\")\n",
    "print(f\"Group C (n={len(data_C)}): M = {mean_C:.2f}, SD = {std_C:.2f}\")\n",
    "print(f\".   Difference (C - B): {mean_difference:.2f}\")\n",
    "print(f\".   Percentage increase from B to C: {percent_increase:.2f}%\")\n",
    "\n",
    "alpha = 0.05\n",
    "# Normality test\n",
    "print(\"\\n=== NORMALITY TESTS (Shapiro-Wilk) ===\")\n",
    "shapiro_B = stats.shapiro(data_B)\n",
    "shapiro_C = stats.shapiro(data_C)\n",
    "print(f\"Group B: p-value = {shapiro_B.pvalue:.4f} -> \"\n",
    "        f\"{'normally distributed' if shapiro_B.pvalue >= alpha else 'not normally distributed'}\")\n",
    "print(f\"Group C: p-value = {shapiro_C.pvalue:.4f} -> \"\n",
    "        f\"{'normally distributed' if shapiro_C.pvalue >= alpha else 'not normally distributed'}\")\n",
    "\n",
    "# Levene test\n",
    "print(\"\\n=== HOMOGENEITY OF VARIANCES (Levene) ===\")\n",
    "levene_p = stats.levene(data_B, data_C).pvalue\n",
    "\n",
    "print(f\"Levene p-value = {levene_p:.4f} -> \"\n",
    "        f\"{'variances are similar' if levene_p >= alpha else 'variances differ'}\")\n",
    "\n",
    "print(f\"\\n=== TEST RESULTS ===\")\n",
    "# Choose appropriate test\n",
    "normal_B = shapiro_B.pvalue >= alpha\n",
    "normal_C = shapiro_C.pvalue >= alpha\n",
    "equal_var = levene_p >= alpha\n",
    "\n",
    "if normal_B and normal_C and equal_var:\n",
    "    print(\"Using independent t-test (parametric)\")\n",
    "    stat, p_value = stats.ttest_ind(data_B, data_C, equal_var=True)\n",
    "    # Cohen's d\n",
    "    n1, n2 = len(data_B), len(data_C)\n",
    "    pooled_std = np.sqrt(((n1 - 1) * std_B**2 + (n2 - 1) * std_C**2) / (n1 + n2 - 2))\n",
    "    cohens_d = (mean_C - mean_B) / pooled_std\n",
    "    \n",
    "    print(f\"Test statistic (t) = {stat:.4f}\")\n",
    "    print(f\"P-value = {p_value:.4f}\")\n",
    "    print(f\"Cohen's d = {cohens_d:.4f}\")\n",
    "    \n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect_label = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect_label = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect_label = \"medium\"\n",
    "    else:\n",
    "        effect_label = \"large\"\n",
    "    print(f\"Effect size: {effect_label}\")\n",
    "\n",
    "else:\n",
    "    print(\"Using Mann-Whitney U test (non-parametric)\")\n",
    "    stat, p_value = stats.mannwhitneyu(data_B, data_C, alternative='two-sided')\n",
    "    # Effect size r = Z / sqrt(N)\n",
    "    n_total = len(data_B) + len(data_C)\n",
    "    z_score = stats.norm.ppf(1 - p_value / 2)  # two-tailed\n",
    "    r_effect = abs(z_score) / np.sqrt(n_total)\n",
    "    \n",
    "    print(f\"Test statistic (U) = {stat:.4f}\")\n",
    "    print(f\"P-value = {p_value:.4f}\")\n",
    "    print(f\"Effect size (r) = {r_effect:.4f}\")\n",
    "    \n",
    "    if r_effect < 0.1:\n",
    "        effect_label = \"negligible\"\n",
    "    elif r_effect < 0.3:\n",
    "        effect_label = \"small\"\n",
    "    elif r_effect < 0.5:\n",
    "        effect_label = \"medium\"\n",
    "    else:\n",
    "        effect_label = \"large\"\n",
    "    print(f\"Effect size: {effect_label}\")\n",
    "\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(f\"SIGNIFICANT difference: p = {p_value:.4f} < {alpha}\")\n",
    "else:\n",
    "    print(f\"No significant difference: p = {p_value:.4f} >= {alpha}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### 6.5/ Action distribution in progression sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "What type of actions do students perform before achieving a progression in the game following feedback depending on the groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_display = 10\n",
    "groups = [int_const.GROUP_B, int_const.GROUP_C]\n",
    "alpha = 0.05\n",
    "\n",
    "# Collect all progression actions across both groups\n",
    "all_actions_overall = []\n",
    "for group in groups:\n",
    "    group_data = analysis_df[(analysis_df['group'] == group) & \n",
    "                             (analysis_df['sequence_outcome'] == 'progression')]\n",
    "    for seq in group_data['actions_sequence']:\n",
    "        all_actions_overall.extend(seq)\n",
    "\n",
    "# Determine global top-k actions\n",
    "overall_counts = pd.Series(all_actions_overall).value_counts()\n",
    "top_actions_global = overall_counts.head(top_n_display).index.tolist()\n",
    "print(f\"Global Top-{top_n_display} actions: {top_actions_global}\")\n",
    "\n",
    "# Prepare counts and percentages per group\n",
    "percentage_df = pd.DataFrame(index=top_actions_global, columns=groups, dtype=float)\n",
    "counts_dict = {}\n",
    "totals_dict = {}\n",
    "significant_flags = {}\n",
    "\n",
    "for group in groups:\n",
    "    group_data = analysis_df[(analysis_df['group'] == group) & \n",
    "                             (analysis_df['sequence_outcome'] == 'progression')]\n",
    "    group_actions = []\n",
    "    for seq in group_data['actions_sequence']:\n",
    "        group_actions.extend(seq)\n",
    "    \n",
    "    action_counts = pd.Series(group_actions).value_counts()\n",
    "    total_actions = len(group_actions)\n",
    "    \n",
    "    counts_dict[group] = np.array([action_counts.get(act, 0) for act in top_actions_global])\n",
    "    totals_dict[group] = total_actions\n",
    "    \n",
    "    print(f\"\\n=============================\")\n",
    "    print(f\"GROUP {group} ACTION DISTRIBUTION (Top {top_n_display})\")\n",
    "    print(f\"=============================\")\n",
    "    for i, action in enumerate(top_actions_global):\n",
    "        pct = (counts_dict[group][i] / total_actions) * 100 if total_actions > 0 else 0\n",
    "        percentage_df.loc[action, group] = pct\n",
    "        print(f\"   {action}: {counts_dict[group][i]} times ({pct:.1f}%)\")\n",
    "    print(f\"   Total actions analyzed: {total_actions}\")\n",
    "\n",
    "# Chi-square test across top actions\n",
    "contingency_table = np.array([counts_dict[int_const.GROUP_B], counts_dict[int_const.GROUP_C]]).T\n",
    "chi2, p_chi2, dof, expected = chi2_contingency(contingency_table)\n",
    "print(\"\\n=== Chi-square Test (B vs C) ===\")\n",
    "print(f\"Chi2 statistic = {chi2:.2f}, p-value = {p_chi2:.4f}\")\n",
    "\n",
    "# Z-tests for top-k actions with Bonferroni correction\n",
    "print(\"\\n=== Z-test for proportions by top action (Bonferroni corrected) ===\")\n",
    "num_tests = len(top_actions_global)\n",
    "for i, action in enumerate(top_actions_global):\n",
    "    counts_i = np.array([counts_dict[int_const.GROUP_B][i], counts_dict[int_const.GROUP_C][i]])\n",
    "    nobs = np.array([totals_dict[int_const.GROUP_B], totals_dict[int_const.GROUP_C]])\n",
    "    \n",
    "    stat, pval = proportions_ztest(counts_i, nobs)\n",
    "    adjusted_p = min(pval * num_tests, 1.0)  # Bonferroni correction\n",
    "    significance = adjusted_p < alpha\n",
    "    significant_flags[action] = significance\n",
    "    label = \"SIGNIFICANT\" if significance else \"not significant\"\n",
    "    print(f\"{action}: Z = {stat:.2f}, raw p-value = {pval:.4f}, adjusted p-value = {adjusted_p:.4f} -> {label}\")\n",
    "\n",
    "# Overall interpretation\n",
    "print(\"\\nOverall interpretation:\")\n",
    "if p_chi2 < alpha:\n",
    "    print(\"The overall distribution of progression actions differs significantly between B and C.\")\n",
    "else:\n",
    "    print(\"No significant overall difference between B and C.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_plot = 9  # number of top actions to display\n",
    "groups = [int_const.GROUP_B, int_const.GROUP_C]\n",
    "\n",
    "# Slice the precomputed results\n",
    "display_actions = percentage_df.index[:top_n_plot]  # top actions to show\n",
    "percentage_df_plot = percentage_df.loc[display_actions, groups]\n",
    "significant_flags_plot = {action: significant_flags.get(action, False) for action in display_actions}\n",
    "\n",
    "# X positions for the bars\n",
    "x = np.arange(len(display_actions))\n",
    "width = 0.4\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "bars_B = ax.bar(x - width/2, percentage_df_plot[int_const.GROUP_B], width, label='Group B', color='skyblue')\n",
    "bars_C = ax.bar(x + width/2, percentage_df_plot[int_const.GROUP_C], width, label='Group C', color='salmon')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(display_actions, rotation=30, ha='right')\n",
    "ax.set_xlabel(\"Action type\", fontsize=12)\n",
    "ax.set_ylabel(\"Percentage (%)\", fontsize=12)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax.set_ylim(0, max(percentage_df_plot.max()) + 5)\n",
    "\n",
    "# Add percentage labels and significance stars\n",
    "for i, action in enumerate(display_actions):\n",
    "    height_B = percentage_df_plot[int_const.GROUP_B][action]\n",
    "    height_C = percentage_df_plot[int_const.GROUP_C][action]\n",
    "    \n",
    "    # Labels on top of each bar\n",
    "    ax.annotate(f'{height_B:.1f}%', xy=(x[i] - width/2, height_B), xytext=(0,3),\n",
    "                textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "    ax.annotate(f'{height_C:.1f}%', xy=(x[i] + width/2, height_C), xytext=(0,3),\n",
    "                textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Add significance star centered between the bars\n",
    "    if significant_flags_plot[action]:\n",
    "        y_star = max(height_B, height_C) + 2\n",
    "        ax.text(x[i], y_star, '*', ha='center', va='bottom', fontsize=16, color='black')\n",
    "\n",
    "# Add legend and layout\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/action_types_BC.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### 6.6/ Action patterns in progression sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "What are the most frequent action patterns in progression sequences depending on groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_display = 10\n",
    "groups = [int_const.GROUP_B, int_const.GROUP_C]\n",
    "alpha = 0.05\n",
    "\n",
    "# Filter only progression events for groups B and C\n",
    "progress_B = analysis_df[(analysis_df['group'] == int_const.GROUP_B) &\n",
    "                         (analysis_df['sequence_outcome'] == 'progression')]\n",
    "progress_C = analysis_df[(analysis_df['group'] == int_const.GROUP_C) &\n",
    "                         (analysis_df['sequence_outcome'] == 'progression')]\n",
    "\n",
    "# Extract full action sequences as tuples\n",
    "sequences_B = progress_B['actions_sequence'].apply(tuple)\n",
    "sequences_C = progress_C['actions_sequence'].apply(tuple)\n",
    "\n",
    "# Get top N sequences overall (based on combined counts)\n",
    "all_sequences = pd.concat([sequences_B, sequences_C])\n",
    "top_sequences_overall = all_sequences.value_counts().head(top_n_display).index.tolist()\n",
    "\n",
    "# Prepare counts and totals for each group\n",
    "counts_dict = {}\n",
    "totals_dict = {}\n",
    "percentage_dict = {}\n",
    "significant_flags = {}\n",
    "\n",
    "for group, sequences in zip(groups, [sequences_B, sequences_C]):\n",
    "    counts = np.array([sequences.tolist().count(seq) for seq in top_sequences_overall])\n",
    "    total = len(sequences)\n",
    "    percentages = 100 * counts / total if total > 0 else np.zeros_like(counts)\n",
    "    \n",
    "    counts_dict[group] = counts\n",
    "    totals_dict[group] = total\n",
    "    percentage_dict[group] = percentages\n",
    "\n",
    "# Display results\n",
    "for group in groups:\n",
    "    print(f\"\\n==================================\")\n",
    "    print(f\"Top {top_n_display} Action Sequences  Group {group}\")\n",
    "    print(f\"==================================\")\n",
    "    for i, seq in enumerate(top_sequences_overall):\n",
    "        count = counts_dict[group][i]\n",
    "        pct = percentage_dict[group][i]\n",
    "        print(f\"- {list(seq)} -> {count} occurrences ({pct:.1f}%)\")\n",
    "\n",
    "\n",
    "# Z-tests for proportions with Bonferroni correction\n",
    "print(\"\\n=== Z-test for proportions by top action sequence ===\")\n",
    "num_tests = len(top_sequences_overall)\n",
    "\n",
    "for i, seq in enumerate(top_sequences_overall):\n",
    "    counts_i = np.array([counts_dict[int_const.GROUP_B][i], counts_dict[int_const.GROUP_C][i]])\n",
    "    nobs = np.array([totals_dict[int_const.GROUP_B], totals_dict[int_const.GROUP_C]])\n",
    "    \n",
    "    stat, pval = proportions_ztest(counts_i, nobs)\n",
    "    adjusted_p = min(pval * num_tests, 1.0)\n",
    "    significance = adjusted_p < alpha\n",
    "    significant_flags[seq] = significance\n",
    "    \n",
    "    label = \"SIGNIFICANT\" if significance else \"not significant\"\n",
    "    print(f\"{list(seq)}: Z = {stat:.2f}, raw p-value = {pval:.4f}, adjusted p-value = {adjusted_p:.4f} -> {label}\")\n",
    "\n",
    "# Overall chi-square test\n",
    "contingency_table = np.array([counts_dict[int_const.GROUP_B],\n",
    "                              counts_dict[int_const.GROUP_C]]).T\n",
    "chi2, p_chi2, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(\"\\n=== Chi-square Test (B vs C) ===\")\n",
    "print(f\"Chi2 statistic = {chi2:.2f}, p-value = {p_chi2:.4f}\")\n",
    "if p_chi2 < alpha:\n",
    "    print(\"The overall distribution of top action sequences differs significantly between B and C.\")\n",
    "else:\n",
    "    print(\"No significant overall difference between B and C.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_plot = 6\n",
    "groups = [int_const.GROUP_B, int_const.GROUP_C]\n",
    "\n",
    "# Slice the top sequences and percentages for plotting\n",
    "display_sequences = top_sequences_overall[:top_n_plot]\n",
    "\n",
    "# Percentages for each group\n",
    "percent_B = [percentage_dict[int_const.GROUP_B][i] for i in range(top_n_plot)]\n",
    "percent_C = [percentage_dict[int_const.GROUP_C][i] for i in range(top_n_plot)]\n",
    "\n",
    "# X positions\n",
    "x = np.arange(top_n_plot)\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Plot bars\n",
    "bars_B = ax.bar(x - width/2, percent_B, width, label='Group B', color='skyblue')\n",
    "bars_C = ax.bar(x + width/2, percent_C, width, label='Group C', color='salmon')\n",
    "\n",
    "# X-axis labels: join actions in sequence\n",
    "labels = ['\\n'.join(seq) for seq in display_sequences]\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=30, ha='right')\n",
    "\n",
    "# Axis labels\n",
    "ax.set_xlabel(\"Action Sequences\", fontsize=12)\n",
    "ax.set_ylabel(\"Percentage (%)\", fontsize=12)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax.set_ylim(0, max(max(percent_B), max(percent_C)) + 5)\n",
    "\n",
    "# Add percentage labels and significance stars\n",
    "for i, seq in enumerate(display_sequences):\n",
    "    height_B = percent_B[i]\n",
    "    height_C = percent_C[i]\n",
    "    \n",
    "    # Percentage labels\n",
    "    ax.annotate(f'{height_B:.1f}%',\n",
    "                xy=(x[i] - width/2, height_B),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    ax.annotate(f'{height_C:.1f}%',\n",
    "                xy=(x[i] + width/2, height_C),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Significance star if sequence is significant\n",
    "    if significant_flags[seq]:\n",
    "        y_star = max(height_B, height_C) + 2\n",
    "        ax.text(x[i], y_star, '*', ha='center', va='bottom', fontsize=16, color='black')\n",
    "\n",
    "# Legend and layout\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/action_sequences_BC.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
